# 大数据分析与管理技术

##  关于大数据

　　*2008年，英国著名学术杂志《Nature》上推出了大数据的专刊。*

　　*美国一些知名数据管理领域的专家从专业角度出发联合发布了一份名为《大数据的机遇与挑战》（Challenges and opportunities with big data）的白皮书，从学术角度介绍了大数据的产生、处理流程和所面临的若干挑战。*

　　*在工业界，全球知名的咨询公司麦肯锡公司（McKinsey）2011年也发表了一份名为《大数据：下一个创新、竞争和生产力的前沿》（Big Data: the next frontier for innovation, competition and productivity）的详尽报告，对大数据带来的巨大影响、关键技术和应用领域进行了详尽阐述和分析。*

　　*美国奥巴马政府更是在2012年发布了“大数据研究和发展倡议”（Big data research and development initiative），斥资2亿多美元计划在科研、环境、生物医学等领域利用大数据分析管理技术取得新的突破。我国政府也于2015年发布了《中共中央关于制定国民经济和社会发展第十三个五年规划的建议》提出实施国家大数据战略，超前布局下一代互联网。*

　　*目前“大数据”（Big data）已成为一个炙手可热的名词。从表面上看，其表示数据规模的庞大，但仅仅从数据规模上无法区分“大数据”这一概念和以往的“海量数据”（Massive data）和“超大规模数据”（Very large data）等概念的区别。然而，至今仍没有一个对“大数据”公认的准确定义。*

   *根据维基百科的解释，**大数据指的是数据规模巨大到无法通过目前主流的软件工具在合理时间内完成处理的数据集。**由此可见，随着时间的推移，计算机的计算能力、存储能力会不断提升，因此大数据的含义也会不断演化。*

   *今天所说的大数据在未来可能就不算“大”了，但那时也一定仍会存在更大的数据是当时的技术处理不了的，因而仍会被称为大数据。可见，如何驾驭大数据将会成为人们长期需要面对的数据常态。*

 

 

+ 　　大数据的“4V”特性：

（1）体量大（Volume）。大数据体现在数据量极为庞大，其计量单位可以是TB级、PB级甚至更大的计量单位。

（2）速度快（Velocity）。大数据呈现出高速增长的态势，而且产生速度仍在不断加快。

（3）多样化（Variety）。大数据包含多种多样的数据类型，既可以是存储在二维表中的结构化数据，也可以是文本、视频、图像、语音、图（Graph）、文件等非结构化数据。

（4）价值高（Value）。大数据中蕴藏着巨大价值，但价值密度低。通过对大数据进行合理的分析，能够从中挖掘出很多有价值的信息，这些信息将有助于提高社会生产效率，提升人们生活质量，或者创造更大商业价值。

 

##  大数据存储技术

​     随着大数据时代的到来，传统关系型数据库的发展面对大数据时代的数据管理需求越来越力不从心，主要体现在：

•        **无法保证对大数据的查询效率**：在大数据时代，短短的1分钟时间内新浪微博可以产生2万条微博，苹果可以产生4.7万次应用下载记录，淘宝则可以卖出6万件商品，百度可以产生90万次搜索记录。可见对于上述公司而言很快就会积累超过10亿的数据量。然而，由于关系模型严谨得过于死板，例如复杂的事务处理机制就成为了阻碍其性能提升的桎梏，使得传统关系型数据库在一张包含10亿条记录的数据表之上进行SQL查询时效率极低。

•        **无法应对繁多的数据类型**：关系型数据库存储的是清洁规整的结构化数据，然而在大数据时代，数据种类繁多，包括文本、图片、音频和视频在内的非结构化数据所占比重更是超过了90%，这无疑是关系型数据库所不能应对的。

•        **横向可扩展能力不足**：传统关系型数据库由于自身设计机理的原因，通常很难实现性价比较高的“横向扩展”，即基于普通廉价的服务器扩充现有分布式计算系统，使系统的处理能力和存储能力得到提升。而“横向扩展”是大数据时代计算和存储的重要需求。

•        **很难满足数据高并发访问需求**：大数据时代诸如购物记录、搜索记录、朋友圈消息等信息都需要实时更新，这就会导致高并发的数据访问，可能产生每秒高达上万次的读写请求。在这种情况下，传统关系型数据库引以为傲的事务处理机制和包括语法分析和性能优化在内的查询优化机制却阻碍了其在并发性能方面的表现。

 

 

　　在以上大数据时代的数据管理需求的推动下，各种新型的NoSQL数据库不断涌现，一方面弥补了关系型数据库存在的各种缺陷，另一方面也撼动了关系型数据库的传统垄断地位。

　　NoSQL（Not only SQL）不是指某个具体的数据库，是对非关系型数据库的统称。

　　NoSQL数据库采用类似键/值、列族、文档和图（Graph）等非关系数据模型，通常没有固定的表结构，没有复杂的查询优化机制，也没有严格的事务ACID特性的约束，因此和关系型数据库相比，NoSQL数据库具有更优秀的查询效率，更灵活的横向可扩展性和更高并发处理性，并能够存储和处理非结构化数据。

 

　　根据所采用的数据模型的不同，NoSQL数据库具体又可以分为以下四类：

•     键值（Key-Value）存储数据库。这一类数据库主要使用哈希表作为数据索引，哈希表中有一个特定的键和一个指针指向特定的数据。这种数据库的优势是简单、易部署、查询速度快；缺点是数据无结构，通常只被当作字符串或者二进制数据。Tokyo Cabinet/Tyrant、Redi、Voldemort、Oracle BDB等都属于这一类数据库。

•     列存储数据库。这类数据库通常用来应对分布式存储海量数据的存储需求。在列存储数据库中，数据以列簇式存储，将同一列数据存在一起。这种数据库的优点是查找速度快，可扩展性强，更容易进行分布式扩展；缺点是功能相对局限。Cassandra、HBase、Riak等属于这一类数据库。

•      文档型数据库。这类数据库与键值存储数据库类似，数据按键值（Key-Value）存对的形式进行存储，但与键值存储数据库不同的是，Value为结构化数据。这类数据库适用于Web应用。文档型数据库的优点是对数据结构要求不严格，表结构可变，不需要像关系型数据库一样需要预先定义表结构；缺点是查询性能不高，缺乏统一的查询语法。uCouchDB、MongoDB等属于这一类数据库。

•      图（Graph）数据库。这类数据库专注于构建关系图谱，存储图结构数据，适用于社交网络、推荐系统等。图数据库的优点是能够直接利用图结构相关算法，如最短路径寻址、N度关系查找等；缺点是很多时候需要对整个图做计算才能得出需要的信息。Neo4J、InfoGrid、Infinite Graph等属于这一类数据库。

 

　　NoSQL数据库不受关系模型约束，具有较好的扩展性，很好地弥补了传统关系型数据库的缺陷。但NoSQL数据库并没有一个统一的架构，每一类NoSQL数据库都有各自适用的场景。同时，NoSQL数据库不能严格保证事务的ACID特性，导致数据的一致性和正确性没法保证。而且NoSQL数据库缺乏完备系统的查询优化机制，在复杂查询方面的效率不如关系型数据库。为此，业界又提出了NewSQL数据库。

　　NewSQL 数据库是对各种新的可扩展、高性能数据库的简称，这类数据库不仅具有NoSQL对海量数据的存储管理能力，还保持了传统数据库支持事务ACID和SQL等特性。

　　不同NewSQL数据库的内部架构差异较大，但是有两个共同的特点：都支持关系数据模型；都是用SQL作为其主要的访问接口。

 

　　目前市面上已有的NewSQL数据库有Spanner、PostgreSQL、SAP HANA、VoltDB、MemSQL等。

Ø  Spanner是谷歌公司研发的、可扩展的、多版本、全球分布式、同步复制数据库，是谷歌公司第一个可以全球扩展并支持数据外部一致性的数据库。

Ø  PostgreSQL是很受欢迎的开源数据库，稳定性强，有大量的几何、字典、数组等数据类型，在地理信息系统领域处于优势地位。

Ø  SAP HANA基于内存计算技术，是面向企业分析性应用的产品，主要包括内存计算引擎和HANA建模工具两部分。

Ø  VoltDB是基于内存的关系型数据库，其采用NewSQL体系架构，既追求与NoSQL体系架构系统具有相匹配的系统可展性，又维护了传统关系型数据库系统的事务特性和SQL语言访问特性，在执行高速并发事务时比传统的关系型数据库系统快45倍。

Ø  MemSQL有符合ACID特性的事务处理功能、SQL兼容性以及高度优化的SQL存储引擎，提供了与MySQL相同的编程接口，但速度比MySQL快30倍。

Ø  还有一些在云端提供存取服务的NewSQL数据库（亦可称为云数据库），例如Amazon RDS和Microsoft SQL Azure。

 

## 大数据处理模式

　　大数据具有数据体量大、产生速度快的特点，因而传统的单机串行处理模式往往难以完成对大数据的高效处理，必须借助并行分布式处理方法。根据大数据应用类型的不同，大数据处理模式分为**批处理**（Batch processing）和**流处理**（Stream processing）两种。下面以Apache的Hadoop和Storm为例分别介绍批处理和流处理的典型处理模式。

 

1. 分布式批处理模式的代表——Hadoop

　　批处理则是对数据先存储后统一处理。Hadoop是一个由Apache基金会用Java语言开发的开源分布式批处理架构，其中实现了MapReduce批处理编程模型。Google公司在2004年提出的MapReduce编程模式是最具代表性的分布式数据批处理模式。

　　MapReduce模型包含三种角色：Master进程、Map进程和Reduce进程，其中Master进程负责任务的划分与调度，Map进程用于执行Map任务，Reduce进程用于执行Reduce任务。该模型的主要思想是Master进程把大规模的数据划分成多个较小的部分，分别映射到多个Map进程进行并行处理得到中间结果，之后由Reduce进程对这些中间结果进行规约、整理，进而得到最终结果，如图1.4所示。

 ![](https://raw.githubusercontent.com/ZanderZhao/images/master/img2019/20191025204750.png)

　　一个MapReduce作业（MapReduce Job）的执行流程如下：

（1）首先从用户提交的程序创建出Master进程，Master进程启动后划分任务并根据输入文件所在位置和集群信息选择机器创建出Map进程或Reduce进程；

（2）Master进程将划分好的任务分配给Map进程和Reduce进程执行，任务划分和任务分配可以并行执行；

（3）Map进程执行Map任务，即读取相应的输入文件，根据指定的输入格式不断地读取<key, value>对，并对每一个<key, value>对执行用户自定义的Map函数；

（4）Map进程不断往本地内存缓冲区输出中间<key, value>对结果，等到缓冲区超过一定大小时写入到本地磁盘中，Map进程将中间结果组织成文件，便于后续Reduce进程获取；

（5）Map任务执行完成后向Master进程汇报，Master进程进一步将该消息通知Reduce进程。Reduce进程向Map进程请求传输生成的中间结果数据，当Reduce进程获取完所有的中间结果后，需要进行排序操作；

（6）Reduce进程执行Reduce任务，即对中间结果的每一个相同的key及value集合，执行用户自定义的Reduce函数，Reduce函数的输出结果被写入到最终的输出文件。

 

　　除了使用MapReduce批处理编程框架，Hadoop的核心内容还包括HDFS（Hadoop Distributed File System，Hadoop分布式文件系统）。

　　HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，适合处理大规模数据集的应用程序。

　　Hadoop的优点包含以下几个方面：

（1）方便部署。Hadoop可以方便部署在由一般商用机器构成的大型集群或者云计算服务之上。

（2）容错健壮。即使集群中的计算机硬件频繁出现失效，Hadoop也能够处理大多数此类故障。

（3）容易扩展。Hadoop通过增加集群节点，可以线性地扩展以处理更大的数据集。

（4）使用简单。Hadoop允许用户快速编写出高效的并行代码。

（5）免费、开源。Hadoop是一款开源批处理框架，可以免费使用。

​      Hadoop的典型应用包括网络搜索、日志处理、推荐系统、数据分析、视频图像分析和数据集成等。

 

2. 分布式流处理模式的代表——Storm

　　和先存储再处理的批处理模式不同，流处理将源源不断产生的数据视为数据流，每当新的数据到达系统时就立刻对数据进行处理并返回结果。可见，流处理适合包括网页点击数统计、股票交易数据分析和传感器网络事件检测等实时分析应用。Apache Storm是一个免费、开源的分布式实时流处理系统。Storm在流处理中的地位相当于Hadoop对于批处理的重要地位。

 　　Storm基于拓扑（Topology）实现对数据流的分布式实时处理。拓扑是一个有向无环图（Directed Acyclic Graph），一个典型的Storm的拓扑结构如图1.5所示。

　　Topology中数据以元组（Tuple）的形式进行转发和处理。和Hadoop中的MapReduce作业不同，Storm的拓扑一经启动将永久运行，不断处理实时到达的数据元组。

 ![](https://raw.githubusercontent.com/ZanderZhao/images/master/img2019/20191025204818.png)

　　Storm拓扑由Spout和Bolt两类组件构成。Spout作为数据产生者，从一个外部源（例如Kafka）读取数据并并向Storm拓扑中喷射数据元组。Bolt作为数据消费者，对所接收的数据元组进行处理和转发。

　　一个复杂的Storm拓扑可由多个Spout和多个Bolt组成，且可以为每个Spout或Bolt设置其任务（Task）并行度，由多个任务并行完成其处理逻辑。Storm提供多种组件间的数据分发策略，例如随机分组（Shuffle grouping）、按字段分组（Field grouping）、全局分组（Global grouping）和广播发送（All grouping），用以完成Storm拓扑中上游组件的各个任务向下游组件的各个任务的数据分发.

　　Storm的优点包含以下几个方面：

（1）易整合：Storm可以方便与数据库系统进行整合。

（2）易使用：Storm提供丰富的API，方便用户的使用。

（3）易扩展：Storm可以方便部署和运行在大规模分布式集群中。

（4）易纠错：Storm可以自行重启故障节点，并完成对故障节点任务的重新分配。

（5）可靠的消息处理：Storm保证每个消息都能被系统完整处理。

（6）免费、开源：Storm是一款开源流处理框架，可以免费使用。

 

 

##  大数据处理的基本流程

#### 1. 数据集成

​       大数据的一个重要特性就是多样化（Variety），这意味着产生大数据的来源广泛、类型庞杂、并经常存在数据冗余和错漏现象，给数据处理带来了巨大挑战。要想处理大数据，首要任务就是对数据源抽取的数据进行合理的集成。

​       数据集成是指通过访问、解析、规范化、标准化、整合、清洗、抽取、匹配、分类、修饰和数据交付等功能把不同来源、格式、特点、性质的数据在逻辑上或物理上有机地集中，从而为后期数据处理提供保障。**数据集成的目的是保证数据的质量和可信性。**如果数据集成工作没有做好，会导致整个大数据项目的延期甚至失败。因此，在大数据给人们带来价值之前，必须对其进行合理的集成。

 

#### 2. 数据分析

​        数据分析是整个大数据处理流程中的核心环节，因为大数据所蕴含的价值需要通过数据分析得以实现。传统的数据分析技术包括数据挖掘、机器学习、统计分析等在用于处理大数据时可能需要进行必要的调整，因为这些技术在处理大数据时面临一些新的挑战，体现在以下几个方面：

　　大数据价值大（Value）的特性虽然意味着大数据蕴含了巨大价值，但是大数据同时也存在价值密度低的特点，体现在大数据中存在大量的冗余数据、噪音数据、遗漏数据和错误数据。因此，在进行数据分析之前，需要对大数据进行数据清洗、整合等集成工作。然而，对如此大规模的数据进行清洗和整合无疑会对硬件环境和算法性能提出新的要求。

 

#### 3. 数据解释

​        虽然数据分析是大数据处理的核心，但是用户更关注对分析结果的展示。即使分析过程高效、分析结果正确，如果没有通过容易理解的方式给用户展示大数据的分析结果将会大大降低分析结果的实际价值，极端情况下甚至会误导用户。

​       传统的数据解释方法是在电脑终端上打印显示分析结果或以文本的形式向用户呈现分析结果。然而，大数据的分析结果往往规模大而且结果之间的关系错综复杂，因而传统的数据解释方法不适用于解释大数据的分析结果。

​       目前，业界推出了很多数据可视化技术，用图表等形象的方式向用户展现大数据的分析结果。常见的数据可视化技术包括标签云（Tag cloud）、历史流（History flow）和空间信息流（Spatial information flow）等。

 

